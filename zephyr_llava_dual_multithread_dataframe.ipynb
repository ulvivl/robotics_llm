{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65ccfd-bb82-481f-88a4-a39c2975047f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc2d34-ca91-4f0d-842b-6a2b38a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "!export CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27132e2d-ddaf-4bde-b9f3-2dc69ac2456a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a10176-ada4-423c-8770-bdd352a9079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d9324-6587-40e4-a8aa-90b83388d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c0d50-82e3-4580-b946-b760e374ec16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c305c-fc40-4993-ba78-ed2e9176ab37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98681a51-886f-47b7-ad30-8de9fabe7b27",
   "metadata": {},
   "source": [
    "## Parallel exec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0ed50-903f-4422-96c1-65cdfa73b7a4",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb410fcc-acaf-4a3c-b936-d365ddd81901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_json('/data/vkarlov/robotics/train_dataset/train_dataset.json')\n",
    "# df['image'] = '/data/vkarlov/robotics/train_dataset/train_images/' + df['image']\n",
    "\n",
    "df = pd.read_json('/home/edamirov/notebooks/ml_hack/playground_pipeline/test_dataset/test_dataset.json')\n",
    "df['image'] = '/home/edamirov/notebooks/ml_hack/playground_pipeline/test_dataset/test_images/train_images/' + df['image']\n",
    "\n",
    "\n",
    "# df = pd.read_csv('df_mistral_questions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515d784-7e21-4bde-8984-ba068bebf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6ba28-e50b-4afd-8364-01dd5e4359ef",
   "metadata": {},
   "source": [
    "### Infer first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e653ab-e991-4cf4-b697-807e57e81a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOT_SYSTEM = \"\"\"\n",
    "You are helpful robot with arm operating in a house. You will be asked to do various tasks and you should tell me the sequence of actions you would do to accomplish my task. \n",
    "You have 3 possible actions: \"pick_up(from, what)\", \"move_to(where, what)\" and \"put(where, what)\". \n",
    "\n",
    "You should ask from 3 to 5 SHORT questions about surroundings to make precise instructions.\n",
    "Pay special attention to whether you are holding any object already, objects locations and robot current location and state - ask relevant questions about all this things.\n",
    "You must work only with objects and locations mentioned in request!\n",
    "IMPORTANT: you must take into account whether you need to move your arm or yourself and if neccessary item is already in your arm!\n",
    "DO NOT SOLVE TASK OR LIST INSTRUCTIONS, JUST ASK QUESTIONS!\n",
    "Example (DON'T REPEAT THIS QUESTIONS!):\n",
    "\n",
    "USER: \n",
    "How would you take a cucumber from the table and put it in an orange box?\n",
    "ASSISTANT: \n",
    "1. Is there anything robot holding now? \n",
    "2. What robot is holding?\n",
    "3. Is orange box far away or nearby?\n",
    "4. Is there anything is the box?\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8303d-9dd9-4160-b06e-bd0222be092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\", max_new_tokens=256, batch_size=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295b38a-4afe-46b5-8ef7-a88f3d18dc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_prompts_1_stage = []\n",
    "for cur_query in tqdm(df['goal_eng'].to_list()):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": ROBOT_SYSTEM,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": cur_query\n",
    "        },\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    query_prompts_1_stage.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1989343-3b00-455d-a48d-bcb9c98fe20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_1_stage_res = pipe(query_prompts_1_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b3b26-390f-4fc0-ab54-329b0fd9ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_questions = [i[0]['generated_text'].split('<|assistant|>\\n')[1] for i in mistral_1_stage_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f62030-493e-4a6c-97af-7dfc124e6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mistral_questions'] = mistral_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44276566-5a23-4d04-8975-58ce4f398fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('df_mistral_questions_TEST.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30dced-1a13-49fc-a6ac-1f34d2e65e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c01118f5-1e3d-4ef9-b119-c6719c4a377a",
   "metadata": {},
   "source": [
    "### Parallel LLaVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3033611-1937-46b8-8729-70b3522e6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import itertools\n",
    "from transformers import ViltProcessor\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "!export CUDA_VISIBLE_DEVICES='5'\n",
    "\n",
    "import torch\n",
    "torch.cuda.device_count()\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf270e-bfbe-4711-8517-1b79866b5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018360e-900f-41c0-9226-58fb0dec5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Step:\n",
    "    action: str = \"\"\n",
    "    text: str = \"\"\n",
    "    arguments: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class SorterTask():\n",
    "    action: str = \"\"\n",
    "    image: str = \"\"\n",
    "    text: str = \"\"\n",
    "    goal: str = \"\"\n",
    "    text: str = \"\"\n",
    "    task_type: int = -1\n",
    "    plan_id: int = -1\n",
    "    steps: List[Step] = field(default_factory=list)\n",
    "    arguments: List[str] = field(default_factory=list)\n",
    "\n",
    "    def to_list(self):\n",
    "        return [[step.action, [arg for arg in step.arguments]] for step in self.steps]\n",
    "\n",
    "class SorterDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        # with open(path_to_csv, 'r') as f:\n",
    "        #     self._data = json.load(f)\n",
    "        self._data = df\n",
    "        self._size = len(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "\n",
    "    def __getitem__(self, idx) -> SorterTask:\n",
    "        entry = self._data[idx]\n",
    "        steps = []\n",
    "        # for plan in entry['plan']:\n",
    "        #     steps.append(Step(action=plan[0],\n",
    "        #                       arguments=plan[1]))\n",
    "        return SorterTask(goal=entry['mistral_questions'],\n",
    "                        image=entry['image'],\n",
    "                        # image='/data/vkarlov/robotics/train_dataset/train_images/' + \"1808600344675605482_0.png\",\n",
    "                        steps=steps,\n",
    "                        task_type=entry['task_type'],\n",
    "                        plan_id=entry[\"plan_id\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0406f-6d34-4578-9dd3-1198034677c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "from transformers import pipeline\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseInput:\n",
    "    text: Optional[str] = None\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class BaseOutput:\n",
    "    text: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6060a1-3618-4120-9551-e8866268ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "conv_mode = 'llava_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674186d2-86a0-4705-9e6a-923db7c19c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None, \n",
    "    model_name=model_name, \n",
    "    load_8bit=True, \n",
    "    load_4bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53dc93-0f89-4602-8912-0190fc5a539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama-2' in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "if conv_mode is not None and conv_mode != conv_mode:\n",
    "    print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, conv_mode, conv_mode))\n",
    "else:\n",
    "    conv_mode = conv_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a0dd3-3f80-4d80-a280-1287c79e5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAVA_SYSTEM = \\\n",
    "'''\n",
    "You are acting as vision module of robot with arm.\n",
    "You are responsible for recognising objects and locations around you.\n",
    "Answer on all questions based on given picture, mention any details important for robot.\n",
    "\n",
    "Pay special attention to whether there is something in your arm or not!\n",
    "Your arm is black, surrounded with red element.\n",
    "IMPORTANT: Your arm is in the middle of the image!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c621696-328d-410b-99c8-eaaf410d9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "def preprocess_text_to_inp_ids(req):\n",
    "    inp = f\"Request: How whould you {req.lower()[:-1]}?\\nAnswer:\"\n",
    "    inp = LLAVA_SYSTEM + inp\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    roles = conv.roles\n",
    "    inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').cuda()\n",
    "    # print(input_ids.shape)\n",
    "    # print(prompt)\n",
    "    return input_ids\n",
    "\n",
    "def pad_ids(input_ids, max_batch_len):\n",
    "    p1d = (max_batch_len - len(input_ids), 0)\n",
    "    return F.pad(input_ids, p1d, \"constant\", 0)\n",
    "\n",
    "def create_att_mask(input_ids, max_batch_len):\n",
    "    return torch.cat((\n",
    "        torch.zeros(max_batch_len - len(input_ids)), torch.ones(len(input_ids))\n",
    "        ))\n",
    "\n",
    "def infer_model(batch):\n",
    "    batch_images = [load_image(elem.image) for elem in batch]\n",
    "    image_tensors = image_processor.preprocess(batch_images, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "    # print(image_tensors)\n",
    "    batch_input_ids = [preprocess_text_to_inp_ids(elem.goal) for elem in batch]\n",
    "    max_batch_len = np.max([len(ids) for ids in batch_input_ids])\n",
    "    batch_att_masks = [create_att_mask(ids, max_batch_len) for ids in batch_input_ids]\n",
    "    batch_input_ids = [pad_ids(ids, max_batch_len) for ids in batch_input_ids]\n",
    "    # print(batch_att_masks[0])\n",
    "    # print(batch_input_ids[0])\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    roles = conv.roles\n",
    "\n",
    "    # print(prompt)\n",
    "    # print('-' * 100)\n",
    "\n",
    "    # stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    # keywords = [stop_str]\n",
    "    # stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, batch_input_ids[0])\n",
    "    # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=torch.stack(batch_input_ids),\n",
    "            attention_mask=torch.stack(batch_att_masks),\n",
    "            images=image_tensors,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=1024,\n",
    "            # streamer=streamer,\n",
    "            use_cache=True,\n",
    "            # stopping_criteria=[stopping_criteria]\n",
    "            )\n",
    "\n",
    "    return [tokenizer.decode(output_ids[i, batch_input_ids[i].shape[0]:], skip_special_tokens=True).strip() for i in range(len(batch))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a65ae3-5685-487f-92cd-daa52e3032c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "class PromptProcessor():\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        self.TERMINATING_STRING = 'done()'\n",
    "        self._system_prompt = \"\"\n",
    "        self._stop_step_pattern = \"\"\n",
    "        self._stop_pattern = re.compile(f'\\\\d+\\\\. {self.TERMINATING_STRING}.')\n",
    "\n",
    "    @property\n",
    "    def system_prompt_is_set(self) -> bool:\n",
    "        return len(self._system_prompt) > 0\n",
    "\n",
    "    def is_terminating(self, step: Step) -> bool:\n",
    "        return step.text == self.TERMINATING_STRING\n",
    "\n",
    "    def build_system_prompt(self, example_tasks: List[SorterTask]) -> str:\n",
    "        prompt = \"Robot: Hi there, I’m a robot operating in a house.\\n\"\n",
    "        prompt += \"Robot: You can ask me to do various tasks and \"\n",
    "        prompt += \"I’ll tell you the sequence of actions I would do to accomplish your task.\\n\"\n",
    "\n",
    "        for task in example_tasks:\n",
    "            prompt += self._task_to_prompt(task) + '\\n'\n",
    "\n",
    "        self._system_prompt = prompt\n",
    "        self._stop_step_pattern = re.compile(\n",
    "            r'(\\s*\\d+\\.\\s*)(\\w+\\((\"[\\w ]+\"(,\\s)?)*\\))*')\n",
    "\n",
    "    def load_prompt_from_file(self, filepath: str) -> None:\n",
    "        with open(filepath, 'r') as file:\n",
    "            self._system_prompt = file.read()\n",
    "        self._stop_step_pattern = re.compile(\n",
    "            r'(\\s*\\d+\\.\\s*)(\\w+\\((\"[\\w ]+\"(,\\s)?)*\\))*')\n",
    "\n",
    "    def _goal_to_query(self, goal: str) -> str:\n",
    "        query = f\"Human: How would you {goal.lower()}?\\n\"\n",
    "        query += f'Robot: '\n",
    "        return query\n",
    "\n",
    "    def _step_to_text(self, step: Step) -> str:\n",
    "        arguments = [f'\"{argument}\"' for argument in step.arguments]\n",
    "        text = f'{step.action}({\", \".join(arguments)})'\n",
    "        return text\n",
    "\n",
    "    def _steps_to_text(self,\n",
    "                       steps: List[Step],\n",
    "                       add_terminating_string: bool = True) -> str:\n",
    "        text = \", \".join([f'{step_idx}. {self._step_to_text(step)}'\n",
    "                          for step_idx, step in enumerate(steps, start=1)])\n",
    "        if add_terminating_string:\n",
    "            text += f\", {len(steps) + 1}. {self.TERMINATING_STRING}.\"\n",
    "        return text\n",
    "\n",
    "    def _task_to_prompt(self, task: SorterTask) -> str:\n",
    "        prompt = self._goal_to_query(task.goal)\n",
    "        text = self._steps_to_text(task.steps)\n",
    "        task.text = text\n",
    "        prompt += text\n",
    "        return prompt\n",
    "\n",
    "    def to_inputs(self,\n",
    "                  task: SorterTask,\n",
    "                  steps: Optional[List[Step]] = None,\n",
    "                  options: Optional[List[Step]] = None) -> BaseInput:\n",
    "        if not self.system_prompt_is_set:\n",
    "            raise ValueError(\n",
    "                \"System prompt is not set. You need to set system prompt.\")\n",
    "        else:\n",
    "            text = self._system_prompt + self._goal_to_query(task.goal)\n",
    "            if steps is not None:\n",
    "                text += self._steps_to_text(steps, add_terminating_string=False)\n",
    "            if options is not None:\n",
    "                return ScoringInput(text=text, options=[f'{len(steps) + 1}. {option.text}' for option in options])\n",
    "            return BaseInput(text=text)\n",
    "\n",
    "    def _text_to_steps(self, task_text: str, cut_one_step: bool = False) -> Union[List[Step], Step, None]:\n",
    "        self._stop_step_pattern = re.compile(r'(\\s*\\d+\\.\\s*)(\\w+\\((\"[\\w ]+\"(,\\s)?)*\\))*')\n",
    "\n",
    "        if cut_one_step:\n",
    "            stop_match = self._stop_step_pattern.match(task_text)\n",
    "            if stop_match is None:\n",
    "                return None\n",
    "            else:\n",
    "                return self._parse_action(stop_match.group(2))\n",
    "        else:\n",
    "            stop_match = self._stop_step_pattern.findall(task_text)\n",
    "            steps = []\n",
    "            if stop_match is None:\n",
    "                return steps\n",
    "            else:\n",
    "                for i in range(len(stop_match) - 1):\n",
    "                    step_text = stop_match[i][1]\n",
    "                    step = self._parse_action(step_text)\n",
    "                    if step is not None:\n",
    "                        steps.append(step)\n",
    "                return steps\n",
    "\n",
    "    def _parse_action(self, step_text: str) -> Optional[Step]:\n",
    "        \"\"\" Parse action with arguments to step.\n",
    "        text: put_on('pepper', 'white box')\n",
    "        action: put_on\n",
    "        arguments: ['pepper', 'white box']\n",
    "        \"\"\"\n",
    "        step_decomposition_pattern = re.compile(r'\\s*([A-Za-z_][A-Za-z_\\s]+)')\n",
    "        arguments = step_decomposition_pattern.findall(step_text)\n",
    "\n",
    "        if arguments is None:\n",
    "            return None\n",
    "        if len(arguments) == 1:\n",
    "            step = Step(text=step_text)\n",
    "        else:\n",
    "            step = Step(action=arguments[0],\n",
    "                        arguments=arguments[1:],\n",
    "                        text=step_text)\n",
    "            return step\n",
    "\n",
    "    def to_task(self, task: BaseOutput) -> SorterTask:\n",
    "        # Full plan generation mode\n",
    "        stop_match = self._stop_pattern.search(task.text)\n",
    "\n",
    "        if stop_match is not None:\n",
    "            task.text = task.text[:stop_match.end() + 2].strip(' \\n\\t')\n",
    "        else:\n",
    "            task.text = task.text.strip(' \\n\\t')\n",
    "\n",
    "        steps = self._text_to_steps(task_text=task.text)\n",
    "\n",
    "        return SorterTask(text=task.text, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3015b-079b-4707-852c-f340e9add79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class FullPlanGeneration():\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 processor,\n",
    "                 **kwargs):\n",
    "        self._processor = processor\n",
    "        self._model = model\n",
    "\n",
    "    def predict(self, gt_task_batch: List[SorterTask]) -> List[SorterTask]:\n",
    "        batch_outputs = infer_model(gt_task_batch)\n",
    "        # model_ouputs = [self._processor.to_task(BaseOutput(text.replace('\\\\', ''))) for text in batch_outputs]\n",
    "        return batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72935e-f828-47b5-bb0f-b7560b09ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_mistral_questions_TEST.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fd12c-2a00-49b7-9d12-2b56441937ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df['image'] = df['image'].apply(lambda img_path: f'/home/edamirov/notebooks/ml_hack/playground_pipeline/test_dataset/test_images/{Path(img_path).name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6bb73-e0c2-49a4-8aec-a23f5ccae2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# path_to_csv = \"/data/vkarlov/robotics/train_dataset/train_dataset.json\"\n",
    "# dataset = SorterDataset(path_to_csv=path_to_csv)\n",
    "dataset = SorterDataset(df=df.to_dict('records'))\n",
    "\n",
    "print(dataset[0])\n",
    "# print(dataset[1])\n",
    "# print(dataset[2])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=16,\n",
    "        collate_fn=lambda x: x,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1abf40-f526-4ed6-8175-980a170513fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = PromptProcessor()\n",
    "# processor.build_system_prompt([dataset[i] for i in range(10)])\n",
    "# print(processor._system_prompt)\n",
    "gen_method = FullPlanGeneration(model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f3af4-2afe-4bd8-a050-56fbc9171a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    # print(batch)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        batch[i].text = processor._steps_to_text(batch[i].steps)\n",
    "        \n",
    "    batch_predict = gen_method.predict(batch)\n",
    "    # print(batch_predict)\n",
    "    for elem, pred_elem in zip(batch, batch_predict):\n",
    "        results.append(\n",
    "            {\n",
    "                'plan_id' : elem.plan_id,\n",
    "                'plan' : pred_elem,\n",
    "                'predicted_text' : pred_elem,\n",
    "                'goal' : elem.goal,\n",
    "                'image' : elem.image,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70478725-efb7-4589-ba7f-8ef11ca08c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c49c20-2ecf-4a00-932c-6bbc7ecf661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4583a-392f-4ad9-8113-be6d1a354f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# with open('llava_results_copy_1_df_dump.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274830a3-a9d4-4720-a028-782537ea21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb57be-9364-4dea-82e3-42c23bd287ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335535e-898f-4a9c-8edc-e08fa6b38eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b55d28-c3d3-46ee-a36f-f75220138842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llava_response'] = pd.DataFrame(results)['predicted_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82415371-0fef-40fb-a2b5-d656e07022ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfdd8957-2c85-4394-8211-12a88f74c949",
   "metadata": {},
   "source": [
    "## Infer mistral final iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67241555-d68f-4269-8125-fc87a6c1498e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca59a64-a384-44b4-acfb-bd6f7899a0ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_prompts_2_stage = []\n",
    "for cur_row in tqdm(df.to_dict('records')):\n",
    "    ROBOT_SYSTEM_2 = f\"\"\"\n",
    "    You are writing instructions for a helpful robot operating in a house. You will be asked to do various tasks and you should tell me the sequence of actions you would do to accomplish my task. You have 3 possible actions: \"pick_up(from, what)\", \"move_to(where, what)\" and \"put(where, what)\". At the end of sequence you should write \"done()\".\n",
    "    Short surroundings description:\n",
    "    {cur_row['llava_response']}\n",
    "    \n",
    "    You have 3 possible actions: \"pick_up(from, what)\", \"move_to(where, what)\" and \"put(where, what)\". At the end of sequence you should write \"done()\".\n",
    "    If you need just nearest object, you should use \"pick_up(unspecified, object)\", if you need to move yourself - use \"move_to(location, unspecified)\"\n",
    "    so if the location (where/from) or object(what) are not specified, you should write \"unspecified\" as action parameter.\n",
    "    IMPORTANT: both arguments must not be unspecified at the same time!\n",
    "    IMPORTANT: unspecified may be used only as \"what\" argument for move_to and \"from\" argument for pick_up\n",
    "    \n",
    "    YOU SHOULD ONLY OUTPUT SEQUENCE OF ACTIONS, AVOID ANY COMMENTS.\n",
    "        \n",
    "    Examples:\n",
    "    Short surroundings description: Table with vegetables and a green container and a bedside table.\n",
    "    user: How would you drive up to the bedside table?\n",
    "    assistant: 1. move_to(\"bedside table\", \"unspecified\"), 2. done().\n",
    "    \n",
    "    Short surroundings description: Robot with cat in the manipulator arm.\n",
    "    user: How would you put the gray cat in the orange box?\n",
    "    assistant: 1. move_to(\"orange box\", \"gray cat\"), 2. put(\"orange box\", \"gray cat\"), 3. done().\n",
    "    \n",
    "    Short surroundings description: Cucumber on the table.\n",
    "    user: How would you take a cucumber from the table and put it in an orange box?\n",
    "    assistant: 1. move_to(\"table\", \"cucumber\"), 2. pick_up(\"table\", \"cucumber\"), 3. move_to(\"orange box\", \"cucumber\"), 4. put(\"orange box\", \"cucumber\"), 5. done().\n",
    "    \n",
    "    Short surroundings description: Orange kitten on the floor and a green box.\n",
    "    user: How would you put the orange kitten in the green box?\n",
    "    assistant: 1. move_to(\"unspecified\", \"orange kitten\"), 2. pick_up(\"unspecified\", \"orange kitten\"), 3. move_to(\"green box\", \"orange kitten\"), 4. put(\"green box\", \"orange kitten\"), 5. done().\n",
    "    \n",
    "    Short surroundings description: Toy cat and green container.\n",
    "    user: How would you put the toy cat in the green container?\n",
    "    assistant: 1. move_to(\"green container\", \"toy cat\"), 2. put(\"green container\", \"toy cat\"), 3. done().\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": ROBOT_SYSTEM_2,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": cur_row['goal_eng']\n",
    "        },\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    query_prompts_2_stage.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2baa61-b937-4075-bec6-b82999802884",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_prompts_2_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a462361-8f5b-4cae-a536-97cd4e862ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_prompts_2_stage[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b839b-77a4-4177-b58d-752074dd42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_final_stage_res = pipe(query_prompts_2_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea21f7-fcc3-4b0f-bb98-a9ba823823f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_final_stage_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df22e0-28f5-4da7-85b5-4f4e0a5e0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "df['mistral_final_res'] = pd.DataFrame(chain(*mistral_final_stage_res))['generated_text'].apply(lambda s: s.split('<|assistant|>\\n')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedf8e0-f09a-470e-bbcd-bd2cfcb09783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_mistral_final_res.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d287dd-ad70-4d50-94ba-4605e2885c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a5fd8a0-e5ff-4c12-a6db-f80915f47a27",
   "metadata": {},
   "source": [
    "### Parse res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988fb43-35e4-44b3-aeb1-9c39a87d3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmds_set = ['move_to', 'pick_up', 'put']\n",
    "\n",
    "def parse_cmd_res(cmd_str):\n",
    "    try:\n",
    "        trunc = cmd_str.split('done()')[0]\n",
    "        sep_lines = trunc.split(')')\n",
    "    \n",
    "        \n",
    "        buf_res = []\n",
    "        for cur_cmd in sep_lines:\n",
    "            cur_res_cmd = []\n",
    "            cur_cmd_splt = cur_cmd.split('(')\n",
    "            for i in cmds_set:\n",
    "                if i in cur_cmd_splt[0]:\n",
    "                    cur_res_cmd.append(i)\n",
    "        \n",
    "            if len(cur_res_cmd) == 0:\n",
    "                break\n",
    "            cur_res_cmd.append(list(map(eval, cur_cmd_splt[1].split(','))))\n",
    "        \n",
    "            buf_res.append(cur_res_cmd)\n",
    "    except:\n",
    "        return \"ERROR\"\n",
    "    return buf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd6c2b-435e-4d5e-a317-a99435a0e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_cmd'] = df['mistral_final_res'].apply(parse_cmd_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407dabc8-d8b6-4625-85b9-282de7eb5c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f31f17a5-afeb-4996-9770-d178731b82e9",
   "metadata": {},
   "source": [
    "## Итог\n",
    "В поле parsed_cmd - итоговый результат модели для прогона на датафрейме"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
